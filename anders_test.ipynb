{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "                                \n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig \n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "                               \n",
    "UNK_TOKEN = '[UNK]'\n",
    "SPL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + [UNK_TOKEN]  # special tokens\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "TRAINING_SIZE = 1000\n",
    "VOCABULARY_SIZE = 1000\n",
    "\n",
    "TOKENIZERS = 'BPE'\n",
    "\n",
    "HUGGINGFACE_TOKEN = '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'\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbkak ak \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity\u001b[1;34m(self, input_texts, batch_size)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))])\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:125\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity.<locals>.encode_mask\u001b[1;34m(mask_position)\u001b[0m\n\u001b[0;32m    123\u001b[0m _x[mask_position] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# convert into a sentence\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m _sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# encode\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "scorer.get_perplexity('bkak ak ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('models/model_es_BPE_vs10000_ts1000/checkpoint-3', use_fast=True)\n",
    "\n",
    "# Check if the decoder is set\n",
    "print(tokenizer.backend_tokenizer.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\n",
    "        \"oscar-corpus/oscar\",\n",
    "        language='tr',\n",
    "        deduplicated = True,\n",
    "        streaming=True,\n",
    "        split=\"train\",  # optional, but the dataset only has a train split#\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'text': \"Son yÄ±llarda gÃ¶rÃ¼len ay tutulmalarÄ±na gÃ¶re daha etkili olacaÄŸÄ± sÃ¶ylenen KanlÄ± veya KÄ±rmÄ±zÄ± Ay TutulmasÄ±na saatler kaldÄ±. Bu akÅŸam (27 Temmuz 2018) gerÃ§ekleÅŸecek olan bu tutulmayÄ± Ã¼lkemizin her yerinden uygun hava koÅŸullarÄ±nda rahatlÄ±kla izleyebileceÄŸiz.\\nTek uydumuz olan Ayâ€™Ä±n yÃ¼zeyini dÃ¼nyamÄ±zÄ±n gÃ¶lgesi tamamen kaplayacak. Bu kaplama esnasÄ±nda Ay kÄ±rmÄ±zÄ± bir renk alacaÄŸÄ±ndan bu tutulmaya KanlÄ± veya KÄ±rmÄ±zÄ± Ay TutulmasÄ± diyorlar.\\nÃœlkemizde 20.13 itibariyle baÅŸlayÄ±p gece yarÄ±sÄ± 02.30â€™a kadar sÃ¼recektir. Ancak Ayâ€™Ä±n tam kÄ±rmÄ±zÄ± hale bÃ¼rÃ¼nmesi 23.15 ile 23.30 saatleri arasÄ±nda gerÃ§ekleÅŸecektir. Kuzey Amerika hariÃ§ dÃ¼nyanÄ±n her yerinden izlenecek olan tutulma esnasÄ±nda eÅŸsiz bir doÄŸa olayÄ±na ÅŸahit olurken muhteÅŸem fotoÄŸraflar da Ã§ekilebilecektir.\\nTutulmanÄ±n tam olarak gerÃ§ekleÅŸmesi iÃ§in Ayâ€™Ä±n tamamen kÄ±zÄ±la bÃ¼rÃ¼nmesi gerekiyor. Bu nedenle bu akÅŸam ki gerÃ§ekleÅŸecek tutulma diÄŸer tutulmalara gÃ¶re daha etkili olacak,\\nBu tutulma esnasÄ±nda dolunayÄ±n Ä±ÅŸÄ±ÄŸÄ± hiÃ§ olmadÄ±ÄŸÄ± kadar sÃ¶nÃ¼k olmasÄ± bekleniyor. Tutulma esnasÄ±nda Marsâ€™Ä±n bile Ayâ€™dan daha parlak gÃ¶rÃ¼lebilecektir.\\nBu tutulma sÄ±rasÄ±nda Ayâ€™Ä±n parlaklÄ±ÄŸÄ±nÄ±n sÃ¶nÃ¼k olmasÄ±ndan dolayÄ± Ã§Ä±plak gÃ¶zle tutulmayÄ± izleyebilecek olmamÄ±z ise diÄŸer en ilginÃ§ noktalardan birisidir.\\nBÃ¶ylesine Ã¶nemli bir olayÄ± kaÃ§Ä±rmayÄ±p tecrÃ¼be edilmesi gerektiÄŸini dÃ¼ÅŸÃ¼nÃ¼yorum. Åžimdiden iyi seyirler ðŸ™‚\\nPek Ã§ok kiÅŸisel geliÅŸim kitabÄ±nda olmayan bir bakÄ±ÅŸ aÃ§Ä±sÄ±yla sÃ¼rekli pozitif olmanÄ±n aslÄ±nda negatiflik getirdiÄŸi Ã¼zerine odaklanÄ±yor Mark Manson'Ä±n kitabÄ±...\\nGezgin Bursada gezilecek yerler Seyyah AraÅŸtÄ±r Hayal Et KeÅŸfet kitap Ã¶nerisi edebiyat tedx Bursa Ulu Åžehir hikaye kitap tavsiyesi kitap motivasyon Bursa Ulu Åžehir kitap sever kÃ¼tÃ¼phanem ne okusam popÃ¼ler kitaplar ted talks tedx talks Merinos AtatÃ¼rk KÃ¼ltÃ¼r Merkezi Enerji MÃ¼zesi Tekstil MÃ¼zesi bursa merinos GÃ¶Ã§ MÃ¼zesi\\t\\nArÅŸiv Ay seÃ§in AÄŸustos 2018 (2) Temmuz 2018 (6) Haziran 2018 (3) Nisan 2018 (1) Åžubat 2017 (2) Ocak 2017 (4)\\n1 nisan 1 nisanda neden ÅŸaka yapÄ±lÄ±r 1 nisan nedir 1 nisan tarihi 1 nisan ÅŸaka gÃ¼nÃ¼ nasÄ±l ortaya Ã§Ä±kmÄ±ÅŸtÄ±r AraÅŸtÄ±r Hayal Et KeÅŸfet bestseller bundan yirmi yÄ±l sonra Bursa Bursada gezilecek yerler Bursa Enerji MÃ¼zesi bursa gÃ¶Ã§ mÃ¼zesi bursa gÃ¶Ã§ tarihi mÃ¼zesi bursa merinos Bursa Ulu Åžehir change deÄŸiÅŸim edebiyat Enerji MÃ¼zesi Gezgin GÃ¶Ã§ MÃ¼zesi hikaye ilk adÄ±m kitap kitap sever kitap tavsiyesi kitap Ã¶nerisi kÃ¼tÃ¼phanem Mark Twain Merinos AtatÃ¼rk KÃ¼ltÃ¼r Merkezi motivasyon neden 1 nisanda ÅŸaka yapÄ±lÄ±r ne okusam piÅŸman olmamak iÃ§in popÃ¼ler kitaplar ric elias Seyyah ted talks tedx tedx talks Tekstil MÃ¼zesi Travel Blogger Ulu Åžehir umut ÅŸaka gÃ¼nÃ¼\"}\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def dataset_batch_generator(iterable_dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Generator to yield batches of size `batch_size` from an IterableDataset.\n",
    "\n",
    "    Args:\n",
    "        iterable_dataset (IterableDataset): The dataset to generate batches from.\n",
    "        batch_size (int): The number of examples per batch.\n",
    "\n",
    "    Yields:\n",
    "        list: A batch of examples.\n",
    "    \"\"\"\n",
    "    dataset_iterator = iter(iterable_dataset)\n",
    "    while True:\n",
    "        batch = list(islice(dataset_iterator, batch_size))\n",
    "        if not batch:  # Stop iteration when no more data is available\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "# Example usage with an IterableDataset\n",
    "batch_size = 2  # Number of examples per batch\n",
    "eval_dataset_gen = dataset_batch_generator(dataset, batch_size)\n",
    "\n",
    "# Get the first batch\n",
    "first_batch = next(eval_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 6, 'text': \"Mavi-Mi Sanat Merkezi , MÃ¼zik , Resim ve diÄŸer sanat grubu dersleri. | Ã–zel Hocam - altairservice.ru\\nÄ°zmir KarÅŸÄ±yaka/Ã‡arÅŸÄ±'daki eÄŸitim kurumumuzda , alanÄ±nda profesyonel eÄŸitmenlerimizle enstrÃ¼man , resim ve diger sanat grubu derslerine bekliyoruz!\\nAltairservice.ru Ã¶zel ders verenler ile Ã¶zel ders almak isteyenler arasÄ±nda aracÄ±lÄ±k hizmeti yapan bir platformdur.\"}\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(eval_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 8, 'text': \"anlami-nedir.com'u TÃ¼rkÃ§e dil araÃ§larÄ± sunan bir sÃ¶zlÃ¼ktÃ¼r, yakÄ±n zamanda sadece anlamlar deÄŸil tÃ¼rkÃ§e ingilizce sÃ¶zlÃ¼k, akademik aramalar ve birÃ§ok edebi araÃ§ ile karÅŸÄ±nÄ±za Ã§Ä±kacaktÄ±r.\\nanlami-nedir.com iÃ§eriklerini Ã¶ncelikle TDK'dan sonra ise editÃ¶rlerin kontrolÃ¼nden geÃ§irerek sizlere sunmaktadÄ±r, eÄŸer bir hatalÄ± kÄ±sÄ±m gÃ¶rdÃ¼yseniz lÃ¼tfen iletiÅŸim'e geÃ§iniz\\nSizde TÃ¼rkÃ§emize katkÄ±da bulunmak ve bilinmiyenleri aktarmak isterseniz editÃ¶r olup paylaÅŸÄ±mlarda bulunabilirsiniz.\"}, {'id': 9, 'text': 'Kepez Belediye BaÅŸkanÄ± Hakan TÃ¼tÃ¼ncÃ¼ ile ekibinin 1,5 yÄ±ldÄ±r Ã¼zerinde Ã§alÄ±ÅŸarak hazÄ±rladÄ±ÄŸÄ± Anadolu Oyuncak MÃ¼zesi, Cuma gÃ¼nÃ¼ saat 18.00â€™de dÃ¼zenlenecek tÃ¶renle ziyarete aÃ§Ä±lacak. Dokuma FabrikasÄ±â€™nÄ±n kreÅŸ binasÄ±nÄ±n oyuncak mÃ¼zesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi, bÃ¶lgenin cazibe merkezi haline gelmesine bÃ¼yÃ¼k katkÄ± saÄŸlayacak.\\nKurucusunun BaÅŸkan TÃ¼tÃ¼ncÃ¼, koordinatÃ¶rÃ¼nÃ¼n teknikten sorumlu baÅŸkan yardÄ±mcÄ±sÄ± Sebahat AdanÄ±r, kÃ¼ratÃ¶rÃ¼nÃ¼n Prof. Dr. Nevzat Ã‡evik, teÅŸhir ve tanziminde ise Emrah ÃœnlÃ¼soyâ€™un imzasÄ±nÄ± taÅŸÄ±yan Anadolu Oyuncak MÃ¼zesi, Ã¶ÄŸrenme, eÄŸlence, ailecek buluÅŸma noktasÄ± olmasÄ± itibariyle de sosyalleÅŸme alanÄ± olarak hizmet verecek. Her yaÅŸtan ve her kesimden insanlarÄ±n kendisinden bir ÅŸeyler bulabileceÄŸi 8 bin adet teÅŸhirleriyle birlikte, mÃ¼zecilik kurgusundaki eÄŸitici programlar ve mini bir EXPO 2016â€™yÄ± andÄ±ran Ã¶zel parkÄ±yla da dikkat Ã§eken bir yer kazandÄ±rÄ±lmÄ±ÅŸ oluyor.\\nTÃ¼rkiyeâ€™nin dÃ¶rt bir yanÄ±ndaki mÃ¼zeleri ziyaret ederek mÃ¼zecilikteki bilgi ve birikimlerini arttÄ±ran BaÅŸkan TÃ¼tÃ¼ncÃ¼, yeni eserinin oluÅŸumunda uzun mesailer harcadÄ±. Anadolu Oyuncak MÃ¼zesiâ€™nin kurarken saÄŸlÄ±klÄ± bir toplumun iyi eÄŸitilmiÅŸ, saÄŸlÄ±klÄ± ve mutlu Ã§ocuklarla mÃ¼mkÃ¼n olacaÄŸÄ±na inanarak hareket ettiklerini belirten TÃ¼tÃ¼ncÃ¼, aydÄ±nlÄ±k bir geleceÄŸin tamamen buna baÄŸlÄ± olduÄŸunu vurguladÄ±.\\nAnadolu Oyuncak MÃ¼zesi her yaÅŸ ve kÃ¼ltÃ¼r grubundan vatandaÅŸlarÄ±mÄ±za hitap edecek koleksiyon ve dÃ¼zenlemeler iÃ§erdiÄŸini ifade eden TÃ¼tÃ¼ncÃ¼, â€œDÃ¼nyanÄ±n her yanÄ±nda oynanan oyuncaklardan Ã¶rnekler Ã§okÃ§a varsa da, aÄŸÄ±rlÄ±ÄŸÄ± geleneksel geÃ§miÅŸimizden gelen yerel oyuncaklar oluÅŸturmaktadÄ±r. Burada her yaÅŸtan Ã§ocuklar iÃ§in masum bir tebessÃ¼m dÃ¼nyasÄ± oluÅŸturdukâ€ diye konuÅŸtu.\\nCOPYRIGHTS 2008 ANTALYA FESTIVALS Antalya TanÄ±tÄ±m VakfÄ±na Aittir. (TÃ¼m HaklarÄ± SaklÄ±dÄ±r - All Rights Reserved)\\nwww.antalyafestivals.org internet sitesinde yer alan her tÃ¼rlÃ¼ etkinlik veya aktivite iÃ§in bilgiler, bu konularda hizmet alÄ±nan Ã¼Ã§Ã¼ncÃ¼ kiÅŸi veya kuruluÅŸlardan temin edilmiÅŸ olup, ATAV (Antalya TanÄ±tÄ±m VakfÄ±) tarafÄ±ndan herhangi bir maddi menfaat temin edilmeksizin genel anlamda bilgi vermek amacÄ±yla hazÄ±rlanmÄ±ÅŸtÄ±r. Bu etkinlik veya aktivitelerde oluÅŸan gecikme ve iptallerde ATAV sorumlu deÄŸildir.'}]\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(eval_dataset_gen)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IterableDataset' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'IterableDataset' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits Shape (before masking): torch.Size([2, 5, 10])\n",
      "Labels Shape (before masking): torch.Size([2, 5])\n",
      "Predicted IDs Shape: torch.Size([2, 5])\n",
      "\n",
      "Masked Labels Shape: torch.Size([9])\n",
      "Masked Logits Shape: torch.Size([9, 10])\n",
      "MaskedPredicted IDs Shape: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "vocab_size = 10\n",
    "\n",
    "logits = torch.randn(batch_size, seq_length, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "labels[torch.rand(batch_size, seq_length) < 0.2] = -100\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids, labels\n",
    "\n",
    "pred_ids, test_labels = preprocess_logits_for_metrics(logits, labels)\n",
    "\n",
    "mask = labels != -100\n",
    "masked_labels = labels[mask]\n",
    "masked_logits = logits[mask]\n",
    "masked_pred_ids = pred_ids[mask]\n",
    "\n",
    "print(\"Logits Shape (before masking):\", logits.shape)\n",
    "print(\"Labels Shape (before masking):\", labels.shape)\n",
    "print(\"Predicted IDs Shape:\", pred_ids.shape)\n",
    "print(\"\\nMasked Labels Shape:\", masked_labels.shape)\n",
    "print(\"Masked Logits Shape:\", masked_logits.shape)\n",
    "print(\"MaskedPredicted IDs Shape:\", masked_pred_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 10], got [2, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [2, 10], got [2, 5]"
     ]
    }
   ],
   "source": [
    "cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8523, -0.9910, -0.1719,  1.1800,  1.0248,  0.1380,  0.3227,\n",
       "          -0.8068,  1.1406, -0.9488],\n",
       "         [ 1.2942,  0.7911, -0.1456, -1.6815,  2.0150,  1.0133,  2.6527,\n",
       "          -1.9350, -1.1000, -0.3361],\n",
       "         [ 1.7715,  0.4003,  1.3261,  0.2488, -1.2448,  0.5456, -0.3574,\n",
       "          -0.2697, -1.9683,  0.5351],\n",
       "         [-0.4149,  1.8625, -0.4660,  0.5480,  0.8599, -0.3028,  1.0551,\n",
       "          -1.9711,  0.1437,  1.9225],\n",
       "         [ 0.2250, -0.3303, -1.0802,  0.3086,  0.6722,  0.2848,  0.4837,\n",
       "           0.4721,  0.2418, -2.4609]],\n",
       "\n",
       "        [[ 0.6996,  0.9707,  0.1643,  1.5405, -1.0516,  0.2262,  0.1655,\n",
       "           1.1343,  3.0344,  0.7068],\n",
       "         [ 0.3244, -0.5386,  0.1677,  0.9271,  1.3254,  0.7711,  0.5922,\n",
       "          -0.8381,  1.6199,  0.0336],\n",
       "         [-0.6184, -0.0764,  0.1190, -1.0213, -1.2341,  1.0799, -0.5915,\n",
       "           0.1283, -0.7859,  0.2088],\n",
       "         [ 1.2835, -1.7577,  1.0665,  2.0596, -0.2640, -0.2467,  1.0790,\n",
       "          -0.5742, -0.7920,  0.0297],\n",
       "         [ 1.2887,  0.7945, -0.0561,  0.0933,  1.3219, -0.4399, -0.9960,\n",
       "           0.4129,  0.6518, -1.1656]]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 5, 2, 1, 5],\n",
       "        [5, 5, 8, 1, 8]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1800, 2.6527, 1.7715, 1.9225, 0.6722],\n",
       "        [3.0344, 1.6199, 1.0799, 2.0596, 1.3219]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.gather(-1, pred_ids.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\n",
    "        \"oscar-corpus/oscar\",\n",
    "        language='myv',\n",
    "        deduplicated = True,\n",
    "        streaming=True,\n",
    "        split=\"train\",  # optional, but the dataset only has a train split#\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = 0\n",
    "words = 0\n",
    "for ex in dataset:\n",
    "    n_+=1\n",
    "    words += len(ex['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unshuffled_deduplicated_af', 'unshuffled_deduplicated_als', 'unshuffled_deduplicated_am', 'unshuffled_deduplicated_an', 'unshuffled_deduplicated_ar', 'unshuffled_deduplicated_arz', 'unshuffled_deduplicated_as', 'unshuffled_deduplicated_ast', 'unshuffled_deduplicated_av', 'unshuffled_deduplicated_az', 'unshuffled_deduplicated_azb', 'unshuffled_deduplicated_ba', 'unshuffled_deduplicated_bar', 'unshuffled_deduplicated_bcl', 'unshuffled_deduplicated_be', 'unshuffled_deduplicated_bg', 'unshuffled_deduplicated_bh', 'unshuffled_deduplicated_bn', 'unshuffled_deduplicated_bo', 'unshuffled_deduplicated_bpy', 'unshuffled_deduplicated_br', 'unshuffled_deduplicated_bs', 'unshuffled_deduplicated_bxr', 'unshuffled_deduplicated_ca', 'unshuffled_deduplicated_cbk', 'unshuffled_deduplicated_ce', 'unshuffled_deduplicated_ceb', 'unshuffled_deduplicated_ckb', 'unshuffled_deduplicated_cs', 'unshuffled_deduplicated_cv', 'unshuffled_deduplicated_cy', 'unshuffled_deduplicated_da', 'unshuffled_deduplicated_de', 'unshuffled_deduplicated_diq', 'unshuffled_deduplicated_dsb', 'unshuffled_deduplicated_dv', 'unshuffled_deduplicated_el', 'unshuffled_deduplicated_eml', 'unshuffled_deduplicated_en', 'unshuffled_deduplicated_eo', 'unshuffled_deduplicated_es', 'unshuffled_deduplicated_et', 'unshuffled_deduplicated_eu', 'unshuffled_deduplicated_fa', 'unshuffled_deduplicated_fi', 'unshuffled_deduplicated_fr', 'unshuffled_deduplicated_frr', 'unshuffled_deduplicated_fy', 'unshuffled_deduplicated_ga', 'unshuffled_deduplicated_gd', 'unshuffled_deduplicated_gl', 'unshuffled_deduplicated_gn', 'unshuffled_deduplicated_gom', 'unshuffled_deduplicated_gu', 'unshuffled_deduplicated_he', 'unshuffled_deduplicated_hi', 'unshuffled_deduplicated_hr', 'unshuffled_deduplicated_hsb', 'unshuffled_deduplicated_ht', 'unshuffled_deduplicated_hu', 'unshuffled_deduplicated_hy', 'unshuffled_deduplicated_ia', 'unshuffled_deduplicated_id', 'unshuffled_deduplicated_ie', 'unshuffled_deduplicated_ilo', 'unshuffled_deduplicated_io', 'unshuffled_deduplicated_is', 'unshuffled_deduplicated_it', 'unshuffled_deduplicated_ja', 'unshuffled_deduplicated_jbo', 'unshuffled_deduplicated_jv', 'unshuffled_deduplicated_ka', 'unshuffled_deduplicated_kk', 'unshuffled_deduplicated_km', 'unshuffled_deduplicated_kn', 'unshuffled_deduplicated_ko', 'unshuffled_deduplicated_krc', 'unshuffled_deduplicated_ku', 'unshuffled_deduplicated_kv', 'unshuffled_deduplicated_kw', 'unshuffled_deduplicated_ky', 'unshuffled_deduplicated_la', 'unshuffled_deduplicated_lb', 'unshuffled_deduplicated_lez', 'unshuffled_deduplicated_li', 'unshuffled_deduplicated_lmo', 'unshuffled_deduplicated_lo', 'unshuffled_deduplicated_lrc', 'unshuffled_deduplicated_lt', 'unshuffled_deduplicated_lv', 'unshuffled_deduplicated_mai', 'unshuffled_deduplicated_mg', 'unshuffled_deduplicated_mhr', 'unshuffled_deduplicated_min', 'unshuffled_deduplicated_mk', 'unshuffled_deduplicated_ml', 'unshuffled_deduplicated_mn', 'unshuffled_deduplicated_mr', 'unshuffled_deduplicated_mrj', 'unshuffled_deduplicated_ms', 'unshuffled_deduplicated_mt', 'unshuffled_deduplicated_mwl', 'unshuffled_deduplicated_my', 'unshuffled_deduplicated_myv', 'unshuffled_deduplicated_mzn', 'unshuffled_deduplicated_nah', 'unshuffled_deduplicated_nap', 'unshuffled_deduplicated_nds', 'unshuffled_deduplicated_ne', 'unshuffled_deduplicated_new', 'unshuffled_deduplicated_nl', 'unshuffled_deduplicated_nn', 'unshuffled_deduplicated_no', 'unshuffled_deduplicated_oc', 'unshuffled_deduplicated_or', 'unshuffled_deduplicated_os', 'unshuffled_deduplicated_pa', 'unshuffled_deduplicated_pam', 'unshuffled_deduplicated_pl', 'unshuffled_deduplicated_pms', 'unshuffled_deduplicated_pnb', 'unshuffled_deduplicated_ps', 'unshuffled_deduplicated_pt', 'unshuffled_deduplicated_qu', 'unshuffled_deduplicated_rm', 'unshuffled_deduplicated_ro', 'unshuffled_deduplicated_ru', 'unshuffled_deduplicated_sa', 'unshuffled_deduplicated_sah', 'unshuffled_deduplicated_scn', 'unshuffled_deduplicated_sd', 'unshuffled_deduplicated_sh', 'unshuffled_deduplicated_si', 'unshuffled_deduplicated_sk', 'unshuffled_deduplicated_sl', 'unshuffled_deduplicated_so', 'unshuffled_deduplicated_sq', 'unshuffled_deduplicated_sr', 'unshuffled_deduplicated_su', 'unshuffled_deduplicated_sv', 'unshuffled_deduplicated_sw', 'unshuffled_deduplicated_ta', 'unshuffled_deduplicated_te', 'unshuffled_deduplicated_tg', 'unshuffled_deduplicated_th', 'unshuffled_deduplicated_tk', 'unshuffled_deduplicated_tl', 'unshuffled_deduplicated_tr', 'unshuffled_deduplicated_tt', 'unshuffled_deduplicated_tyv', 'unshuffled_deduplicated_ug', 'unshuffled_deduplicated_uk', 'unshuffled_deduplicated_ur', 'unshuffled_deduplicated_uz', 'unshuffled_deduplicated_vec', 'unshuffled_deduplicated_vi', 'unshuffled_deduplicated_vo', 'unshuffled_deduplicated_wa', 'unshuffled_deduplicated_war', 'unshuffled_deduplicated_wuu', 'unshuffled_deduplicated_xal', 'unshuffled_deduplicated_xmf', 'unshuffled_deduplicated_yi', 'unshuffled_deduplicated_yo', 'unshuffled_deduplicated_yue', 'unshuffled_deduplicated_zh', 'unshuffled_original_af', 'unshuffled_original_als', 'unshuffled_original_am', 'unshuffled_original_an', 'unshuffled_original_ar', 'unshuffled_original_arz', 'unshuffled_original_as', 'unshuffled_original_ast', 'unshuffled_original_av', 'unshuffled_original_az', 'unshuffled_original_azb', 'unshuffled_original_ba', 'unshuffled_original_bar', 'unshuffled_original_bcl', 'unshuffled_original_be', 'unshuffled_original_bg', 'unshuffled_original_bh', 'unshuffled_original_bn', 'unshuffled_original_bo', 'unshuffled_original_bpy', 'unshuffled_original_br', 'unshuffled_original_bs', 'unshuffled_original_bxr', 'unshuffled_original_ca', 'unshuffled_original_cbk', 'unshuffled_original_ce', 'unshuffled_original_ceb', 'unshuffled_original_ckb', 'unshuffled_original_cs', 'unshuffled_original_cv', 'unshuffled_original_cy', 'unshuffled_original_da', 'unshuffled_original_de', 'unshuffled_original_diq', 'unshuffled_original_dsb', 'unshuffled_original_dv', 'unshuffled_original_el', 'unshuffled_original_eml', 'unshuffled_original_en', 'unshuffled_original_eo', 'unshuffled_original_es', 'unshuffled_original_et', 'unshuffled_original_eu', 'unshuffled_original_fa', 'unshuffled_original_fi', 'unshuffled_original_fr', 'unshuffled_original_frr', 'unshuffled_original_fy', 'unshuffled_original_ga', 'unshuffled_original_gd', 'unshuffled_original_gl', 'unshuffled_original_gn', 'unshuffled_original_gom', 'unshuffled_original_gu', 'unshuffled_original_he', 'unshuffled_original_hi', 'unshuffled_original_hr', 'unshuffled_original_hsb', 'unshuffled_original_ht', 'unshuffled_original_hu', 'unshuffled_original_hy', 'unshuffled_original_ia', 'unshuffled_original_id', 'unshuffled_original_ie', 'unshuffled_original_ilo', 'unshuffled_original_io', 'unshuffled_original_is', 'unshuffled_original_it', 'unshuffled_original_ja', 'unshuffled_original_jbo', 'unshuffled_original_jv', 'unshuffled_original_ka', 'unshuffled_original_kk', 'unshuffled_original_km', 'unshuffled_original_kn', 'unshuffled_original_ko', 'unshuffled_original_krc', 'unshuffled_original_ku', 'unshuffled_original_kv', 'unshuffled_original_kw', 'unshuffled_original_ky', 'unshuffled_original_la', 'unshuffled_original_lb', 'unshuffled_original_lez', 'unshuffled_original_li', 'unshuffled_original_lmo', 'unshuffled_original_lo', 'unshuffled_original_lrc', 'unshuffled_original_lt', 'unshuffled_original_lv', 'unshuffled_original_mai', 'unshuffled_original_mg', 'unshuffled_original_mhr', 'unshuffled_original_min', 'unshuffled_original_mk', 'unshuffled_original_ml', 'unshuffled_original_mn', 'unshuffled_original_mr', 'unshuffled_original_mrj', 'unshuffled_original_ms', 'unshuffled_original_mt', 'unshuffled_original_mwl', 'unshuffled_original_my', 'unshuffled_original_myv', 'unshuffled_original_mzn', 'unshuffled_original_nah', 'unshuffled_original_nap', 'unshuffled_original_nds', 'unshuffled_original_ne', 'unshuffled_original_new', 'unshuffled_original_nl', 'unshuffled_original_nn', 'unshuffled_original_no', 'unshuffled_original_oc', 'unshuffled_original_or', 'unshuffled_original_os', 'unshuffled_original_pa', 'unshuffled_original_pam', 'unshuffled_original_pl', 'unshuffled_original_pms', 'unshuffled_original_pnb', 'unshuffled_original_ps', 'unshuffled_original_pt', 'unshuffled_original_qu', 'unshuffled_original_rm', 'unshuffled_original_ro', 'unshuffled_original_ru', 'unshuffled_original_sa', 'unshuffled_original_sah', 'unshuffled_original_scn', 'unshuffled_original_sd', 'unshuffled_original_sh', 'unshuffled_original_si', 'unshuffled_original_sk', 'unshuffled_original_sl', 'unshuffled_original_so', 'unshuffled_original_sq', 'unshuffled_original_sr', 'unshuffled_original_su', 'unshuffled_original_sv', 'unshuffled_original_sw', 'unshuffled_original_ta', 'unshuffled_original_te', 'unshuffled_original_tg', 'unshuffled_original_th', 'unshuffled_original_tk', 'unshuffled_original_tl', 'unshuffled_original_tr', 'unshuffled_original_tt', 'unshuffled_original_tyv', 'unshuffled_original_ug', 'unshuffled_original_uk', 'unshuffled_original_ur', 'unshuffled_original_uz', 'unshuffled_original_vec', 'unshuffled_original_vi', 'unshuffled_original_vo', 'unshuffled_original_wa', 'unshuffled_original_war', 'unshuffled_original_wuu', 'unshuffled_original_xal', 'unshuffled_original_xmf', 'unshuffled_original_yi', 'unshuffled_original_yo', 'unshuffled_original_yue', 'unshuffled_original_zh']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"oscar-corpus/oscar\")\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ah140\\.cache\\huggingface\\hub\\datasets--oscar. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset_config_names\n\u001b[1;32m----> 3\u001b[0m configs \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_config_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moscar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(configs)  \u001b[38;5;66;03m# Lists all possible configurations, including deduplicated ones\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\inspect.py:164\u001b[0m, in \u001b[0;36mget_dataset_config_names\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset_config_names\u001b[39m(\n\u001b[0;32m    109\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    110\u001b[0m     revision: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Version]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_kwargs,\n\u001b[0;32m    116\u001b[0m ):\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the list of available config names for a particular dataset.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path))\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(builder_cls\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;129;01mor\u001b[39;00m [\n\u001b[0;32m    175\u001b[0m         dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, builder_cls\u001b[38;5;241m.\u001b[39mDEFAULT_CONFIG_NAME \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:1729\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1724\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1726\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1727\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1728\u001b[0m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1729\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1732\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1733\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:1679\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m     \u001b[38;5;66;03m# Otherwise we must use the dataset script if the user trusts it\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:1329\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m importable_file_path \u001b[38;5;241m=\u001b[39m _get_importable_file_path(\n\u001b[0;32m   1323\u001b[0m     dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[0;32m   1324\u001b[0m     module_namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1325\u001b[0m     subdirectory_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhash\u001b[39m,\n\u001b[0;32m   1326\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1327\u001b[0m )\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(importable_file_path):\n\u001b[1;32m-> 1329\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1331\u001b[0m         _create_importable_file(\n\u001b[0;32m   1332\u001b[0m             local_path\u001b[38;5;241m=\u001b[39mlocal_path,\n\u001b[0;32m   1333\u001b[0m             local_imports\u001b[38;5;241m=\u001b[39mlocal_imports,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m             download_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_mode,\n\u001b[0;32m   1340\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:138\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, repo_id)\u001b[0m\n\u001b[0;32m    135\u001b[0m         signal\u001b[38;5;241m.\u001b[39malarm(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;66;03m# OS which does not support signal.SIGALRM\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contains custom code which must be executed to correctly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload the dataset. You can inspect the repository content at https://hf.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# For the CI which might put the timeout at 0\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"oscar\")\n",
    "print(configs)  # Lists all possible configurations, including deduplicated ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1002: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m scorer \u001b[38;5;241m=\u001b[39m lmppl\u001b[38;5;241m.\u001b[39mMaskedLM(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/model_es_BPE_vs10000_ts1000/checkpoint-3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m----> 9\u001b[0m ppl \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#print(list(zip(text, ppl)))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# >>> [\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#   ('sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.', 1190212.1699246117),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(f\"prediction: {text[ppl.index(min(ppl))]}\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# >>> \"prediction: sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity\u001b[1;34m(self, input_texts, batch_size)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))])\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:125\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity.<locals>.encode_mask\u001b[1;34m(mask_position)\u001b[0m\n\u001b[0;32m    123\u001b[0m _x[mask_position] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# convert into a sentence\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m _sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# encode\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "import lmppl\n",
    "\n",
    "\n",
    "scorer = lmppl.MaskedLM('models/model_es_BPE_vs10000_ts1000/checkpoint-3')\n",
    "text = [\n",
    "    'sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.',\n",
    "    'sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.'\n",
    "]\n",
    "ppl = scorer.get_perplexity(text)\n",
    "#print(list(zip(text, ppl)))\n",
    "# >>> [\n",
    "#   ('sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.', 1190212.1699246117),\n",
    "#   ('sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.', 1152767.482071837)\n",
    "# ]\n",
    "# print(f\"prediction: {text[ppl.index(min(ppl))]}\")\n",
    "# >>> \"prediction: sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_model_from_checkpoint(model_folder) -> str:\n",
    "\n",
    "    checkpoints = [d for d in os.listdir(model_folder) if d.startswith(\"checkpoint-\")]\n",
    "    print(len(checkpoints))\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {model_folder}\")\n",
    "        exit()\n",
    "\n",
    "    # Find the checkpoint with the highest step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]), reverse=True)\n",
    "\n",
    "    checkpoint_dir = os.path.join(model_folder, checkpoints[0])\n",
    "    print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "models/model_es_BPE_vs10000_ts10\\checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "load_model_from_checkpoint('models/model_es_BPE_vs10000_ts10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m512\u001b[39m])\n\u001b[0;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m10000\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([100,512])\n",
    "logits = torch.tensor([100,512,10000])\n",
    "\n",
    "logits[torch.arange(len(labels)), labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  100,   512, 10000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_LOG_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'models/'\n",
    "tokenizer_folder = \"tokenizers/\"\n",
    "\n",
    "tokenizer_file = f\"{tokenizer_folder}/tokenizer_{TOKENIZERS}_{VOCABULARY_SIZE}_{TRAINING_SIZE}.json\"\n",
    "model_file = f\"{model_folder}/model_{TOKENIZERS}_{VOCABULARY_SIZE}_{TRAINING_SIZE}.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = load_dataset(\"oscar-corpus/oscar\",\n",
    "                        language=\"tr\", \n",
    "                        streaming=True, # optional\n",
    "                        split=\"train\") # optional, but the dataset only has a train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + [UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_text_iterator(dataset):\n",
    "    \"\"\"Yields the 'text' column from an iterable dataset.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataset (IterableDataset): An iterable dataset where each sample \n",
    "                                    is expected to be a dictionary with a \n",
    "                                    'text' field.\n",
    "\n",
    "    Yields:\n",
    "        str: The text content from each sample in the dataset.\n",
    "    \"\"\"\n",
    "    for sample in dataset:\n",
    "        yield sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.take(training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token = UNK_TOKEN))\n",
    "trainer = BpeTrainer(special_tokens = SPL_TOKENS, vocab_size=VOCABULARY_SIZE)\n",
    "\n",
    "tokenizer.train_from_iterator(dataset_text_iterator(dataset), trainer)\n",
    "tokenizer.save(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"mask_token\": \"[MASK]\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples, tokenizer=tokenizer):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        examples (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = DistilBertConfig(vocab_size=VOCABULARY_SIZE)\n",
    "\n",
    "#Distilbert with a Masked language modeling head on top.\n",
    "# only use MLM, does not perform Next-Sentence-Prediction. But this should be sufficient.\n",
    "model = DistilBertForMaskedLM(configuration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ah140\\AppData\\Local\\Temp\\ipykernel_12424\\3130847336.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0cb9c0fd9c4270a52c8ae7905e24f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777286247, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ah140\\OneDrive\\itu\\3_semester\\nlp\\nlp_project\\project\\wandb\\run-20241122_130149-twlp7lgy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ah140797/huggingface/runs/twlp7lgy' target=\"_blank\">first_try</a></strong> to <a href='https://wandb.ai/ah140797/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ah140797/huggingface' target=\"_blank\">https://wandb.ai/ah140797/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ah140797/huggingface/runs/twlp7lgy' target=\"_blank\">https://wandb.ai/ah140797/huggingface/runs/twlp7lgy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7a872816014e8e922ed1b3a874ebcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0439, 'grad_norm': 2.877178430557251, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.33}\n",
      "{'loss': 6.9932, 'grad_norm': 2.752091407775879, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.67}\n",
      "{'loss': 6.9209, 'grad_norm': 2.731001853942871, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\model_BPE_1000_1000.json\\checkpoint-3)... Done. 5.4s\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.6872, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'train_loss': 6.98600435256958, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=6.98600435256958, metrics={'train_runtime': 270.6872, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'total_flos': 6358582296576.0, 'train_loss': 6.98600435256958, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(encode_with_truncation, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{model_file}\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type='linear',\n",
    "    num_train_epochs=3000,\n",
    "    max_steps = 3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    #load_best_model_at_end=True,\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=1, \n",
    "    use_cpu=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name='first_try'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for perplexity contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/perplexity/perplexity.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " torch.Size([3, 5, 10])\n",
      "\n",
      "Labels:\n",
      " torch.Size([3, 5])\n",
      "\n",
      "Computed Metrics: {'perplexity': 20.857810974121094}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the perplexity metric (or any other metric, e.g., accuracy)\n",
    "metric = load_metric(\"perplexity\")\n",
    "\n",
    "# Define a compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    greedy_prediction = torch.argmax(logits, dim=-1)\n",
    "    # Compute loss using cross-entropy\n",
    "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction=\"mean\")\n",
    "    # Calculate perplexity as exp(loss)\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    return {\"perplexity\": perplexity.item()}\n",
    "\n",
    "# Simulate some logits and labels for a batch of predictions\n",
    "# Assuming a vocab size of 10 and a sequence length of 5 for this example\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "vocab_size = 10\n",
    "\n",
    "# Random logits from a model's output (e.g., from a language model)\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Simulated labels (true values) for the same batch and sequence length\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Print logits and labels for reference\n",
    "print(\"Logits:\\n\", logits.shape)\n",
    "print(\"\\nLabels:\\n\", labels.shape)\n",
    "\n",
    "# Call the compute_metrics function to calculate perplexity\n",
    "metrics = compute_metrics((logits, labels))\n",
    "\n",
    "# Print the computed perplexity\n",
    "print(\"\\nComputed Metrics:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedNLP_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
