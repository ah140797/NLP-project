{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "                                \n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig \n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "                               \n",
    "UNK_TOKEN = '[UNK]'\n",
    "SPL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + [UNK_TOKEN]  # special tokens\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "TRAINING_SIZE = 1000\n",
    "VOCABULARY_SIZE = 1000\n",
    "\n",
    "TOKENIZERS = 'BPE'\n",
    "\n",
    "HUGGINGFACE_TOKEN = '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'\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_LOG_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'models/'\n",
    "tokenizer_folder = \"tokenizers/\"\n",
    "\n",
    "tokenizer_file = f\"{tokenizer_folder}/tokenizer_{TOKENIZERS}_{VOCABULARY_SIZE}_{TRAINING_SIZE}.json\"\n",
    "model_file = f\"{model_folder}/model_{TOKENIZERS}_{VOCABULARY_SIZE}_{TRAINING_SIZE}.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = load_dataset(\"oscar-corpus/oscar\",\n",
    "                        language=\"tr\", \n",
    "                        streaming=True, # optional\n",
    "                        split=\"train\") # optional, but the dataset only has a train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + [UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_text_iterator(dataset):\n",
    "    \"\"\"Yields the 'text' column from an iterable dataset.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataset (IterableDataset): An iterable dataset where each sample \n",
    "                                    is expected to be a dictionary with a \n",
    "                                    'text' field.\n",
    "\n",
    "    Yields:\n",
    "        str: The text content from each sample in the dataset.\n",
    "    \"\"\"\n",
    "    for sample in dataset:\n",
    "        yield sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.take(training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token = UNK_TOKEN))\n",
    "trainer = BpeTrainer(special_tokens = SPL_TOKENS, vocab_size=VOCABULARY_SIZE)\n",
    "\n",
    "tokenizer.train_from_iterator(dataset_text_iterator(dataset), trainer)\n",
    "tokenizer.save(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"mask_token\": \"[MASK]\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples, tokenizer=tokenizer):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        examples (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = DistilBertConfig(vocab_size=VOCABULARY_SIZE)\n",
    "\n",
    "#Distilbert with a Masked language modeling head on top.\n",
    "# only use MLM, does not perform Next-Sentence-Prediction. But this should be sufficient.\n",
    "model = DistilBertForMaskedLM(configuration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ah140\\AppData\\Local\\Temp\\ipykernel_12424\\3130847336.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0cb9c0fd9c4270a52c8ae7905e24f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777286247, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ah140\\OneDrive\\itu\\3_semester\\nlp\\nlp_project\\project\\wandb\\run-20241122_130149-twlp7lgy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ah140797/huggingface/runs/twlp7lgy' target=\"_blank\">first_try</a></strong> to <a href='https://wandb.ai/ah140797/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ah140797/huggingface' target=\"_blank\">https://wandb.ai/ah140797/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ah140797/huggingface/runs/twlp7lgy' target=\"_blank\">https://wandb.ai/ah140797/huggingface/runs/twlp7lgy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7a872816014e8e922ed1b3a874ebcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0439, 'grad_norm': 2.877178430557251, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.33}\n",
      "{'loss': 6.9932, 'grad_norm': 2.752091407775879, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.67}\n",
      "{'loss': 6.9209, 'grad_norm': 2.731001853942871, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\model_BPE_1000_1000.json\\checkpoint-3)... Done. 5.4s\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.6872, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'train_loss': 6.98600435256958, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=6.98600435256958, metrics={'train_runtime': 270.6872, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'total_flos': 6358582296576.0, 'train_loss': 6.98600435256958, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(encode_with_truncation, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{model_file}\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type='linear',\n",
    "    num_train_epochs=3000,\n",
    "    max_steps = 3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    #load_best_model_at_end=True,\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=1, \n",
    "    use_cpu=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name='first_try'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for perplexity contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/perplexity/perplexity.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " torch.Size([3, 5, 10])\n",
      "\n",
      "Labels:\n",
      " torch.Size([3, 5])\n",
      "\n",
      "Computed Metrics: {'perplexity': 20.857810974121094}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the perplexity metric (or any other metric, e.g., accuracy)\n",
    "metric = load_metric(\"perplexity\")\n",
    "\n",
    "# Define a compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    greedy_prediction = torch.argmax(logits, dim=-1)\n",
    "    # Compute loss using cross-entropy\n",
    "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction=\"mean\")\n",
    "    # Calculate perplexity as exp(loss)\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    return {\"perplexity\": perplexity.item()}\n",
    "\n",
    "# Simulate some logits and labels for a batch of predictions\n",
    "# Assuming a vocab size of 10 and a sequence length of 5 for this example\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "vocab_size = 10\n",
    "\n",
    "# Random logits from a model's output (e.g., from a language model)\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Simulated labels (true values) for the same batch and sequence length\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Print logits and labels for reference\n",
    "print(\"Logits:\\n\", logits.shape)\n",
    "print(\"\\nLabels:\\n\", labels.shape)\n",
    "\n",
    "# Call the compute_metrics function to calculate perplexity\n",
    "metrics = compute_metrics((logits, labels))\n",
    "\n",
    "# Print the computed perplexity\n",
    "print(\"\\nComputed Metrics:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedNLP_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
