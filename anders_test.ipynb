{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "                                \n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig \n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "                               \n",
    "UNK_TOKEN = '[UNK]'\n",
    "SPL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + [UNK_TOKEN]  # special tokens\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "TRAINING_SIZE = 1000\n",
    "VOCABULARY_SIZE = 1000\n",
    "\n",
    "TOKENIZERS = 'BPE'\n",
    "\n",
    "HUGGINGFACE_TOKEN = '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'\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbkak ak \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity\u001b[1;34m(self, input_texts, batch_size)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))])\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:125\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity.<locals>.encode_mask\u001b[1;34m(mask_position)\u001b[0m\n\u001b[0;32m    123\u001b[0m _x[mask_position] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# convert into a sentence\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m _sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# encode\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "scorer.get_perplexity('bkak ak ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('models/model_es_BPE_vs10000_ts1000/checkpoint-3', use_fast=True)\n",
    "\n",
    "# Check if the decoder is set\n",
    "print(tokenizer.backend_tokenizer.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\n",
    "        \"oscar-corpus/oscar\",\n",
    "        language='tr',\n",
    "        deduplicated = True,\n",
    "        streaming=True,\n",
    "        split=\"train\",  # optional, but the dataset only has a train split#\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'text': \"Son yıllarda görülen ay tutulmalarına göre daha etkili olacağı söylenen Kanlı veya Kırmızı Ay Tutulmasına saatler kaldı. Bu akşam (27 Temmuz 2018) gerçekleşecek olan bu tutulmayı ülkemizin her yerinden uygun hava koşullarında rahatlıkla izleyebileceğiz.\\nTek uydumuz olan Ay’ın yüzeyini dünyamızın gölgesi tamamen kaplayacak. Bu kaplama esnasında Ay kırmızı bir renk alacağından bu tutulmaya Kanlı veya Kırmızı Ay Tutulması diyorlar.\\nÜlkemizde 20.13 itibariyle başlayıp gece yarısı 02.30’a kadar sürecektir. Ancak Ay’ın tam kırmızı hale bürünmesi 23.15 ile 23.30 saatleri arasında gerçekleşecektir. Kuzey Amerika hariç dünyanın her yerinden izlenecek olan tutulma esnasında eşsiz bir doğa olayına şahit olurken muhteşem fotoğraflar da çekilebilecektir.\\nTutulmanın tam olarak gerçekleşmesi için Ay’ın tamamen kızıla bürünmesi gerekiyor. Bu nedenle bu akşam ki gerçekleşecek tutulma diğer tutulmalara göre daha etkili olacak,\\nBu tutulma esnasında dolunayın ışığı hiç olmadığı kadar sönük olması bekleniyor. Tutulma esnasında Mars’ın bile Ay’dan daha parlak görülebilecektir.\\nBu tutulma sırasında Ay’ın parlaklığının sönük olmasından dolayı çıplak gözle tutulmayı izleyebilecek olmamız ise diğer en ilginç noktalardan birisidir.\\nBöylesine önemli bir olayı kaçırmayıp tecrübe edilmesi gerektiğini düşünüyorum. Şimdiden iyi seyirler 🙂\\nPek çok kişisel gelişim kitabında olmayan bir bakış açısıyla sürekli pozitif olmanın aslında negatiflik getirdiği üzerine odaklanıyor Mark Manson'ın kitabı...\\nGezgin Bursada gezilecek yerler Seyyah Araştır Hayal Et Keşfet kitap önerisi edebiyat tedx Bursa Ulu Şehir hikaye kitap tavsiyesi kitap motivasyon Bursa Ulu Şehir kitap sever kütüphanem ne okusam popüler kitaplar ted talks tedx talks Merinos Atatürk Kültür Merkezi Enerji Müzesi Tekstil Müzesi bursa merinos Göç Müzesi\\t\\nArşiv Ay seçin Ağustos 2018 (2) Temmuz 2018 (6) Haziran 2018 (3) Nisan 2018 (1) Şubat 2017 (2) Ocak 2017 (4)\\n1 nisan 1 nisanda neden şaka yapılır 1 nisan nedir 1 nisan tarihi 1 nisan şaka günü nasıl ortaya çıkmıştır Araştır Hayal Et Keşfet bestseller bundan yirmi yıl sonra Bursa Bursada gezilecek yerler Bursa Enerji Müzesi bursa göç müzesi bursa göç tarihi müzesi bursa merinos Bursa Ulu Şehir change değişim edebiyat Enerji Müzesi Gezgin Göç Müzesi hikaye ilk adım kitap kitap sever kitap tavsiyesi kitap önerisi kütüphanem Mark Twain Merinos Atatürk Kültür Merkezi motivasyon neden 1 nisanda şaka yapılır ne okusam pişman olmamak için popüler kitaplar ric elias Seyyah ted talks tedx tedx talks Tekstil Müzesi Travel Blogger Ulu Şehir umut şaka günü\"}\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def dataset_batch_generator(iterable_dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Generator to yield batches of size `batch_size` from an IterableDataset.\n",
    "\n",
    "    Args:\n",
    "        iterable_dataset (IterableDataset): The dataset to generate batches from.\n",
    "        batch_size (int): The number of examples per batch.\n",
    "\n",
    "    Yields:\n",
    "        list: A batch of examples.\n",
    "    \"\"\"\n",
    "    dataset_iterator = iter(iterable_dataset)\n",
    "    while True:\n",
    "        batch = list(islice(dataset_iterator, batch_size))\n",
    "        if not batch:  # Stop iteration when no more data is available\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "# Example usage with an IterableDataset\n",
    "batch_size = 2  # Number of examples per batch\n",
    "eval_dataset_gen = dataset_batch_generator(dataset, batch_size)\n",
    "\n",
    "# Get the first batch\n",
    "first_batch = next(eval_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 6, 'text': \"Mavi-Mi Sanat Merkezi , Müzik , Resim ve diğer sanat grubu dersleri. | Özel Hocam - altairservice.ru\\nİzmir Karşıyaka/Çarşı'daki eğitim kurumumuzda , alanında profesyonel eğitmenlerimizle enstrüman , resim ve diger sanat grubu derslerine bekliyoruz!\\nAltairservice.ru özel ders verenler ile özel ders almak isteyenler arasında aracılık hizmeti yapan bir platformdur.\"}\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(eval_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 8, 'text': \"anlami-nedir.com'u Türkçe dil araçları sunan bir sözlüktür, yakın zamanda sadece anlamlar değil türkçe ingilizce sözlük, akademik aramalar ve birçok edebi araç ile karşınıza çıkacaktır.\\nanlami-nedir.com içeriklerini öncelikle TDK'dan sonra ise editörlerin kontrolünden geçirerek sizlere sunmaktadır, eğer bir hatalı kısım gördüyseniz lütfen iletişim'e geçiniz\\nSizde Türkçemize katkıda bulunmak ve bilinmiyenleri aktarmak isterseniz editör olup paylaşımlarda bulunabilirsiniz.\"}, {'id': 9, 'text': 'Kepez Belediye Başkanı Hakan Tütüncü ile ekibinin 1,5 yıldır üzerinde çalışarak hazırladığı Anadolu Oyuncak Müzesi, Cuma günü saat 18.00’de düzenlenecek törenle ziyarete açılacak. Dokuma Fabrikası’nın kreş binasının oyuncak müzesine dönüştürülmesi, bölgenin cazibe merkezi haline gelmesine büyük katkı sağlayacak.\\nKurucusunun Başkan Tütüncü, koordinatörünün teknikten sorumlu başkan yardımcısı Sebahat Adanır, küratörünün Prof. Dr. Nevzat Çevik, teşhir ve tanziminde ise Emrah Ünlüsoy’un imzasını taşıyan Anadolu Oyuncak Müzesi, öğrenme, eğlence, ailecek buluşma noktası olması itibariyle de sosyalleşme alanı olarak hizmet verecek. Her yaştan ve her kesimden insanların kendisinden bir şeyler bulabileceği 8 bin adet teşhirleriyle birlikte, müzecilik kurgusundaki eğitici programlar ve mini bir EXPO 2016’yı andıran özel parkıyla da dikkat çeken bir yer kazandırılmış oluyor.\\nTürkiye’nin dört bir yanındaki müzeleri ziyaret ederek müzecilikteki bilgi ve birikimlerini arttıran Başkan Tütüncü, yeni eserinin oluşumunda uzun mesailer harcadı. Anadolu Oyuncak Müzesi’nin kurarken sağlıklı bir toplumun iyi eğitilmiş, sağlıklı ve mutlu çocuklarla mümkün olacağına inanarak hareket ettiklerini belirten Tütüncü, aydınlık bir geleceğin tamamen buna bağlı olduğunu vurguladı.\\nAnadolu Oyuncak Müzesi her yaş ve kültür grubundan vatandaşlarımıza hitap edecek koleksiyon ve düzenlemeler içerdiğini ifade eden Tütüncü, “Dünyanın her yanında oynanan oyuncaklardan örnekler çokça varsa da, ağırlığı geleneksel geçmişimizden gelen yerel oyuncaklar oluşturmaktadır. Burada her yaştan çocuklar için masum bir tebessüm dünyası oluşturduk” diye konuştu.\\nCOPYRIGHTS 2008 ANTALYA FESTIVALS Antalya Tanıtım Vakfına Aittir. (Tüm Hakları Saklıdır - All Rights Reserved)\\nwww.antalyafestivals.org internet sitesinde yer alan her türlü etkinlik veya aktivite için bilgiler, bu konularda hizmet alınan üçüncü kişi veya kuruluşlardan temin edilmiş olup, ATAV (Antalya Tanıtım Vakfı) tarafından herhangi bir maddi menfaat temin edilmeksizin genel anlamda bilgi vermek amacıyla hazırlanmıştır. Bu etkinlik veya aktivitelerde oluşan gecikme ve iptallerde ATAV sorumlu değildir.'}]\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(eval_dataset_gen)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IterableDataset' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'IterableDataset' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits Shape (before masking): torch.Size([2, 5, 10])\n",
      "Labels Shape (before masking): torch.Size([2, 5])\n",
      "Predicted IDs Shape: torch.Size([2, 5])\n",
      "\n",
      "Masked Labels Shape: torch.Size([9])\n",
      "Masked Logits Shape: torch.Size([9, 10])\n",
      "MaskedPredicted IDs Shape: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "vocab_size = 10\n",
    "\n",
    "logits = torch.randn(batch_size, seq_length, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "labels[torch.rand(batch_size, seq_length) < 0.2] = -100\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids, labels\n",
    "\n",
    "pred_ids, test_labels = preprocess_logits_for_metrics(logits, labels)\n",
    "\n",
    "mask = labels != -100\n",
    "masked_labels = labels[mask]\n",
    "masked_logits = logits[mask]\n",
    "masked_pred_ids = pred_ids[mask]\n",
    "\n",
    "print(\"Logits Shape (before masking):\", logits.shape)\n",
    "print(\"Labels Shape (before masking):\", labels.shape)\n",
    "print(\"Predicted IDs Shape:\", pred_ids.shape)\n",
    "print(\"\\nMasked Labels Shape:\", masked_labels.shape)\n",
    "print(\"Masked Logits Shape:\", masked_logits.shape)\n",
    "print(\"MaskedPredicted IDs Shape:\", masked_pred_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 10], got [2, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [2, 10], got [2, 5]"
     ]
    }
   ],
   "source": [
    "cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8523, -0.9910, -0.1719,  1.1800,  1.0248,  0.1380,  0.3227,\n",
       "          -0.8068,  1.1406, -0.9488],\n",
       "         [ 1.2942,  0.7911, -0.1456, -1.6815,  2.0150,  1.0133,  2.6527,\n",
       "          -1.9350, -1.1000, -0.3361],\n",
       "         [ 1.7715,  0.4003,  1.3261,  0.2488, -1.2448,  0.5456, -0.3574,\n",
       "          -0.2697, -1.9683,  0.5351],\n",
       "         [-0.4149,  1.8625, -0.4660,  0.5480,  0.8599, -0.3028,  1.0551,\n",
       "          -1.9711,  0.1437,  1.9225],\n",
       "         [ 0.2250, -0.3303, -1.0802,  0.3086,  0.6722,  0.2848,  0.4837,\n",
       "           0.4721,  0.2418, -2.4609]],\n",
       "\n",
       "        [[ 0.6996,  0.9707,  0.1643,  1.5405, -1.0516,  0.2262,  0.1655,\n",
       "           1.1343,  3.0344,  0.7068],\n",
       "         [ 0.3244, -0.5386,  0.1677,  0.9271,  1.3254,  0.7711,  0.5922,\n",
       "          -0.8381,  1.6199,  0.0336],\n",
       "         [-0.6184, -0.0764,  0.1190, -1.0213, -1.2341,  1.0799, -0.5915,\n",
       "           0.1283, -0.7859,  0.2088],\n",
       "         [ 1.2835, -1.7577,  1.0665,  2.0596, -0.2640, -0.2467,  1.0790,\n",
       "          -0.5742, -0.7920,  0.0297],\n",
       "         [ 1.2887,  0.7945, -0.0561,  0.0933,  1.3219, -0.4399, -0.9960,\n",
       "           0.4129,  0.6518, -1.1656]]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 5, 2, 1, 5],\n",
       "        [5, 5, 8, 1, 8]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1800, 2.6527, 1.7715, 1.9225, 0.6722],\n",
       "        [3.0344, 1.6199, 1.0799, 2.0596, 1.3219]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.gather(-1, pred_ids.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\n",
    "        \"oscar-corpus/oscar\",\n",
    "        language='myv',\n",
    "        deduplicated = True,\n",
    "        streaming=True,\n",
    "        split=\"train\",  # optional, but the dataset only has a train split#\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = 0\n",
    "words = 0\n",
    "for ex in dataset:\n",
    "    n_+=1\n",
    "    words += len(ex['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unshuffled_deduplicated_af', 'unshuffled_deduplicated_als', 'unshuffled_deduplicated_am', 'unshuffled_deduplicated_an', 'unshuffled_deduplicated_ar', 'unshuffled_deduplicated_arz', 'unshuffled_deduplicated_as', 'unshuffled_deduplicated_ast', 'unshuffled_deduplicated_av', 'unshuffled_deduplicated_az', 'unshuffled_deduplicated_azb', 'unshuffled_deduplicated_ba', 'unshuffled_deduplicated_bar', 'unshuffled_deduplicated_bcl', 'unshuffled_deduplicated_be', 'unshuffled_deduplicated_bg', 'unshuffled_deduplicated_bh', 'unshuffled_deduplicated_bn', 'unshuffled_deduplicated_bo', 'unshuffled_deduplicated_bpy', 'unshuffled_deduplicated_br', 'unshuffled_deduplicated_bs', 'unshuffled_deduplicated_bxr', 'unshuffled_deduplicated_ca', 'unshuffled_deduplicated_cbk', 'unshuffled_deduplicated_ce', 'unshuffled_deduplicated_ceb', 'unshuffled_deduplicated_ckb', 'unshuffled_deduplicated_cs', 'unshuffled_deduplicated_cv', 'unshuffled_deduplicated_cy', 'unshuffled_deduplicated_da', 'unshuffled_deduplicated_de', 'unshuffled_deduplicated_diq', 'unshuffled_deduplicated_dsb', 'unshuffled_deduplicated_dv', 'unshuffled_deduplicated_el', 'unshuffled_deduplicated_eml', 'unshuffled_deduplicated_en', 'unshuffled_deduplicated_eo', 'unshuffled_deduplicated_es', 'unshuffled_deduplicated_et', 'unshuffled_deduplicated_eu', 'unshuffled_deduplicated_fa', 'unshuffled_deduplicated_fi', 'unshuffled_deduplicated_fr', 'unshuffled_deduplicated_frr', 'unshuffled_deduplicated_fy', 'unshuffled_deduplicated_ga', 'unshuffled_deduplicated_gd', 'unshuffled_deduplicated_gl', 'unshuffled_deduplicated_gn', 'unshuffled_deduplicated_gom', 'unshuffled_deduplicated_gu', 'unshuffled_deduplicated_he', 'unshuffled_deduplicated_hi', 'unshuffled_deduplicated_hr', 'unshuffled_deduplicated_hsb', 'unshuffled_deduplicated_ht', 'unshuffled_deduplicated_hu', 'unshuffled_deduplicated_hy', 'unshuffled_deduplicated_ia', 'unshuffled_deduplicated_id', 'unshuffled_deduplicated_ie', 'unshuffled_deduplicated_ilo', 'unshuffled_deduplicated_io', 'unshuffled_deduplicated_is', 'unshuffled_deduplicated_it', 'unshuffled_deduplicated_ja', 'unshuffled_deduplicated_jbo', 'unshuffled_deduplicated_jv', 'unshuffled_deduplicated_ka', 'unshuffled_deduplicated_kk', 'unshuffled_deduplicated_km', 'unshuffled_deduplicated_kn', 'unshuffled_deduplicated_ko', 'unshuffled_deduplicated_krc', 'unshuffled_deduplicated_ku', 'unshuffled_deduplicated_kv', 'unshuffled_deduplicated_kw', 'unshuffled_deduplicated_ky', 'unshuffled_deduplicated_la', 'unshuffled_deduplicated_lb', 'unshuffled_deduplicated_lez', 'unshuffled_deduplicated_li', 'unshuffled_deduplicated_lmo', 'unshuffled_deduplicated_lo', 'unshuffled_deduplicated_lrc', 'unshuffled_deduplicated_lt', 'unshuffled_deduplicated_lv', 'unshuffled_deduplicated_mai', 'unshuffled_deduplicated_mg', 'unshuffled_deduplicated_mhr', 'unshuffled_deduplicated_min', 'unshuffled_deduplicated_mk', 'unshuffled_deduplicated_ml', 'unshuffled_deduplicated_mn', 'unshuffled_deduplicated_mr', 'unshuffled_deduplicated_mrj', 'unshuffled_deduplicated_ms', 'unshuffled_deduplicated_mt', 'unshuffled_deduplicated_mwl', 'unshuffled_deduplicated_my', 'unshuffled_deduplicated_myv', 'unshuffled_deduplicated_mzn', 'unshuffled_deduplicated_nah', 'unshuffled_deduplicated_nap', 'unshuffled_deduplicated_nds', 'unshuffled_deduplicated_ne', 'unshuffled_deduplicated_new', 'unshuffled_deduplicated_nl', 'unshuffled_deduplicated_nn', 'unshuffled_deduplicated_no', 'unshuffled_deduplicated_oc', 'unshuffled_deduplicated_or', 'unshuffled_deduplicated_os', 'unshuffled_deduplicated_pa', 'unshuffled_deduplicated_pam', 'unshuffled_deduplicated_pl', 'unshuffled_deduplicated_pms', 'unshuffled_deduplicated_pnb', 'unshuffled_deduplicated_ps', 'unshuffled_deduplicated_pt', 'unshuffled_deduplicated_qu', 'unshuffled_deduplicated_rm', 'unshuffled_deduplicated_ro', 'unshuffled_deduplicated_ru', 'unshuffled_deduplicated_sa', 'unshuffled_deduplicated_sah', 'unshuffled_deduplicated_scn', 'unshuffled_deduplicated_sd', 'unshuffled_deduplicated_sh', 'unshuffled_deduplicated_si', 'unshuffled_deduplicated_sk', 'unshuffled_deduplicated_sl', 'unshuffled_deduplicated_so', 'unshuffled_deduplicated_sq', 'unshuffled_deduplicated_sr', 'unshuffled_deduplicated_su', 'unshuffled_deduplicated_sv', 'unshuffled_deduplicated_sw', 'unshuffled_deduplicated_ta', 'unshuffled_deduplicated_te', 'unshuffled_deduplicated_tg', 'unshuffled_deduplicated_th', 'unshuffled_deduplicated_tk', 'unshuffled_deduplicated_tl', 'unshuffled_deduplicated_tr', 'unshuffled_deduplicated_tt', 'unshuffled_deduplicated_tyv', 'unshuffled_deduplicated_ug', 'unshuffled_deduplicated_uk', 'unshuffled_deduplicated_ur', 'unshuffled_deduplicated_uz', 'unshuffled_deduplicated_vec', 'unshuffled_deduplicated_vi', 'unshuffled_deduplicated_vo', 'unshuffled_deduplicated_wa', 'unshuffled_deduplicated_war', 'unshuffled_deduplicated_wuu', 'unshuffled_deduplicated_xal', 'unshuffled_deduplicated_xmf', 'unshuffled_deduplicated_yi', 'unshuffled_deduplicated_yo', 'unshuffled_deduplicated_yue', 'unshuffled_deduplicated_zh', 'unshuffled_original_af', 'unshuffled_original_als', 'unshuffled_original_am', 'unshuffled_original_an', 'unshuffled_original_ar', 'unshuffled_original_arz', 'unshuffled_original_as', 'unshuffled_original_ast', 'unshuffled_original_av', 'unshuffled_original_az', 'unshuffled_original_azb', 'unshuffled_original_ba', 'unshuffled_original_bar', 'unshuffled_original_bcl', 'unshuffled_original_be', 'unshuffled_original_bg', 'unshuffled_original_bh', 'unshuffled_original_bn', 'unshuffled_original_bo', 'unshuffled_original_bpy', 'unshuffled_original_br', 'unshuffled_original_bs', 'unshuffled_original_bxr', 'unshuffled_original_ca', 'unshuffled_original_cbk', 'unshuffled_original_ce', 'unshuffled_original_ceb', 'unshuffled_original_ckb', 'unshuffled_original_cs', 'unshuffled_original_cv', 'unshuffled_original_cy', 'unshuffled_original_da', 'unshuffled_original_de', 'unshuffled_original_diq', 'unshuffled_original_dsb', 'unshuffled_original_dv', 'unshuffled_original_el', 'unshuffled_original_eml', 'unshuffled_original_en', 'unshuffled_original_eo', 'unshuffled_original_es', 'unshuffled_original_et', 'unshuffled_original_eu', 'unshuffled_original_fa', 'unshuffled_original_fi', 'unshuffled_original_fr', 'unshuffled_original_frr', 'unshuffled_original_fy', 'unshuffled_original_ga', 'unshuffled_original_gd', 'unshuffled_original_gl', 'unshuffled_original_gn', 'unshuffled_original_gom', 'unshuffled_original_gu', 'unshuffled_original_he', 'unshuffled_original_hi', 'unshuffled_original_hr', 'unshuffled_original_hsb', 'unshuffled_original_ht', 'unshuffled_original_hu', 'unshuffled_original_hy', 'unshuffled_original_ia', 'unshuffled_original_id', 'unshuffled_original_ie', 'unshuffled_original_ilo', 'unshuffled_original_io', 'unshuffled_original_is', 'unshuffled_original_it', 'unshuffled_original_ja', 'unshuffled_original_jbo', 'unshuffled_original_jv', 'unshuffled_original_ka', 'unshuffled_original_kk', 'unshuffled_original_km', 'unshuffled_original_kn', 'unshuffled_original_ko', 'unshuffled_original_krc', 'unshuffled_original_ku', 'unshuffled_original_kv', 'unshuffled_original_kw', 'unshuffled_original_ky', 'unshuffled_original_la', 'unshuffled_original_lb', 'unshuffled_original_lez', 'unshuffled_original_li', 'unshuffled_original_lmo', 'unshuffled_original_lo', 'unshuffled_original_lrc', 'unshuffled_original_lt', 'unshuffled_original_lv', 'unshuffled_original_mai', 'unshuffled_original_mg', 'unshuffled_original_mhr', 'unshuffled_original_min', 'unshuffled_original_mk', 'unshuffled_original_ml', 'unshuffled_original_mn', 'unshuffled_original_mr', 'unshuffled_original_mrj', 'unshuffled_original_ms', 'unshuffled_original_mt', 'unshuffled_original_mwl', 'unshuffled_original_my', 'unshuffled_original_myv', 'unshuffled_original_mzn', 'unshuffled_original_nah', 'unshuffled_original_nap', 'unshuffled_original_nds', 'unshuffled_original_ne', 'unshuffled_original_new', 'unshuffled_original_nl', 'unshuffled_original_nn', 'unshuffled_original_no', 'unshuffled_original_oc', 'unshuffled_original_or', 'unshuffled_original_os', 'unshuffled_original_pa', 'unshuffled_original_pam', 'unshuffled_original_pl', 'unshuffled_original_pms', 'unshuffled_original_pnb', 'unshuffled_original_ps', 'unshuffled_original_pt', 'unshuffled_original_qu', 'unshuffled_original_rm', 'unshuffled_original_ro', 'unshuffled_original_ru', 'unshuffled_original_sa', 'unshuffled_original_sah', 'unshuffled_original_scn', 'unshuffled_original_sd', 'unshuffled_original_sh', 'unshuffled_original_si', 'unshuffled_original_sk', 'unshuffled_original_sl', 'unshuffled_original_so', 'unshuffled_original_sq', 'unshuffled_original_sr', 'unshuffled_original_su', 'unshuffled_original_sv', 'unshuffled_original_sw', 'unshuffled_original_ta', 'unshuffled_original_te', 'unshuffled_original_tg', 'unshuffled_original_th', 'unshuffled_original_tk', 'unshuffled_original_tl', 'unshuffled_original_tr', 'unshuffled_original_tt', 'unshuffled_original_tyv', 'unshuffled_original_ug', 'unshuffled_original_uk', 'unshuffled_original_ur', 'unshuffled_original_uz', 'unshuffled_original_vec', 'unshuffled_original_vi', 'unshuffled_original_vo', 'unshuffled_original_wa', 'unshuffled_original_war', 'unshuffled_original_wuu', 'unshuffled_original_xal', 'unshuffled_original_xmf', 'unshuffled_original_yi', 'unshuffled_original_yo', 'unshuffled_original_yue', 'unshuffled_original_zh']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"oscar-corpus/oscar\")\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ah140\\.cache\\huggingface\\hub\\datasets--oscar. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset_config_names\n\u001b[1;32m----> 3\u001b[0m configs \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_config_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moscar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(configs)  \u001b[38;5;66;03m# Lists all possible configurations, including deduplicated ones\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\inspect.py:164\u001b[0m, in \u001b[0;36mget_dataset_config_names\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset_config_names\u001b[39m(\n\u001b[0;32m    109\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    110\u001b[0m     revision: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Version]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_kwargs,\n\u001b[0;32m    116\u001b[0m ):\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the list of available config names for a particular dataset.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path))\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(builder_cls\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;129;01mor\u001b[39;00m [\n\u001b[0;32m    175\u001b[0m         dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, builder_cls\u001b[38;5;241m.\u001b[39mDEFAULT_CONFIG_NAME \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:1729\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1724\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1726\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1727\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1728\u001b[0m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1729\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1732\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1733\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:1679\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m     \u001b[38;5;66;03m# Otherwise we must use the dataset script if the user trusts it\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:1329\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m importable_file_path \u001b[38;5;241m=\u001b[39m _get_importable_file_path(\n\u001b[0;32m   1323\u001b[0m     dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[0;32m   1324\u001b[0m     module_namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1325\u001b[0m     subdirectory_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhash\u001b[39m,\n\u001b[0;32m   1326\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1327\u001b[0m )\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(importable_file_path):\n\u001b[1;32m-> 1329\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1331\u001b[0m         _create_importable_file(\n\u001b[0;32m   1332\u001b[0m             local_path\u001b[38;5;241m=\u001b[39mlocal_path,\n\u001b[0;32m   1333\u001b[0m             local_imports\u001b[38;5;241m=\u001b[39mlocal_imports,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m             download_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_mode,\n\u001b[0;32m   1340\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\datasets\\load.py:138\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, repo_id)\u001b[0m\n\u001b[0;32m    135\u001b[0m         signal\u001b[38;5;241m.\u001b[39malarm(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;66;03m# OS which does not support signal.SIGALRM\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contains custom code which must be executed to correctly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload the dataset. You can inspect the repository content at https://hf.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# For the CI which might put the timeout at 0\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"oscar\")\n",
    "print(configs)  # Lists all possible configurations, including deduplicated ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1002: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m scorer \u001b[38;5;241m=\u001b[39m lmppl\u001b[38;5;241m.\u001b[39mMaskedLM(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/model_es_BPE_vs10000_ts1000/checkpoint-3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m----> 9\u001b[0m ppl \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#print(list(zip(text, ppl)))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# >>> [\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#   ('sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.', 1190212.1699246117),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(f\"prediction: {text[ppl.index(min(ppl))]}\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# >>> \"prediction: sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity\u001b[1;34m(self, input_texts, batch_size)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([encode_mask(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_token_prefix), \u001b[38;5;28mlen\u001b[39m(x)))])\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([\u001b[43mencode_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))])\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get partition\u001b[39;00m\n\u001b[0;32m    144\u001b[0m partition \u001b[38;5;241m=\u001b[39m get_partition(data)\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\lmppl\\ppl_mlm.py:125\u001b[0m, in \u001b[0;36mMaskedLM.get_perplexity.<locals>.encode_mask\u001b[1;34m(mask_position)\u001b[0m\n\u001b[0;32m    123\u001b[0m _x[mask_position] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# convert into a sentence\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m _sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# encode\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ah140\\anaconda3_\\envs\\AdvancedNLP_DL\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "import lmppl\n",
    "\n",
    "\n",
    "scorer = lmppl.MaskedLM('models/model_es_BPE_vs10000_ts1000/checkpoint-3')\n",
    "text = [\n",
    "    'sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.',\n",
    "    'sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.'\n",
    "]\n",
    "ppl = scorer.get_perplexity(text)\n",
    "#print(list(zip(text, ppl)))\n",
    "# >>> [\n",
    "#   ('sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am happy.', 1190212.1699246117),\n",
    "#   ('sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.', 1152767.482071837)\n",
    "# ]\n",
    "# print(f\"prediction: {text[ppl.index(min(ppl))]}\")\n",
    "# >>> \"prediction: sentiment classification: I dropped my laptop on my knee, and someone stole my coffee. I am sad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_model_from_checkpoint(model_folder) -> str:\n",
    "\n",
    "    checkpoints = [d for d in os.listdir(model_folder) if d.startswith(\"checkpoint-\")]\n",
    "    print(len(checkpoints))\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {model_folder}\")\n",
    "        exit()\n",
    "\n",
    "    # Find the checkpoint with the highest step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]), reverse=True)\n",
    "\n",
    "    checkpoint_dir = os.path.join(model_folder, checkpoints[0])\n",
    "    print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "models/model_es_BPE_vs10000_ts10\\checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "load_model_from_checkpoint('models/model_es_BPE_vs10000_ts10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m512\u001b[39m])\n\u001b[0;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m10000\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([100,512])\n",
    "logits = torch.tensor([100,512,10000])\n",
    "\n",
    "logits[torch.arange(len(labels)), labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  100,   512, 10000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_LOG_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'models/'\n",
    "tokenizer_folder = \"tokenizers/\"\n",
    "\n",
    "tokenizer_file = f\"{tokenizer_folder}/tokenizer_{TOKENIZERS}_{VOCABULARY_SIZE}_{TRAINING_SIZE}.json\"\n",
    "model_file = f\"{model_folder}/model_{TOKENIZERS}_{VOCABULARY_SIZE}_{TRAINING_SIZE}.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = load_dataset(\"oscar-corpus/oscar\",\n",
    "                        language=\"tr\", \n",
    "                        streaming=True, # optional\n",
    "                        split=\"train\") # optional, but the dataset only has a train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + [UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_text_iterator(dataset):\n",
    "    \"\"\"Yields the 'text' column from an iterable dataset.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataset (IterableDataset): An iterable dataset where each sample \n",
    "                                    is expected to be a dictionary with a \n",
    "                                    'text' field.\n",
    "\n",
    "    Yields:\n",
    "        str: The text content from each sample in the dataset.\n",
    "    \"\"\"\n",
    "    for sample in dataset:\n",
    "        yield sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.take(training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token = UNK_TOKEN))\n",
    "trainer = BpeTrainer(special_tokens = SPL_TOKENS, vocab_size=VOCABULARY_SIZE)\n",
    "\n",
    "tokenizer.train_from_iterator(dataset_text_iterator(dataset), trainer)\n",
    "tokenizer.save(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"mask_token\": \"[MASK]\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples, tokenizer=tokenizer):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        examples (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = DistilBertConfig(vocab_size=VOCABULARY_SIZE)\n",
    "\n",
    "#Distilbert with a Masked language modeling head on top.\n",
    "# only use MLM, does not perform Next-Sentence-Prediction. But this should be sufficient.\n",
    "model = DistilBertForMaskedLM(configuration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ah140\\AppData\\Local\\Temp\\ipykernel_12424\\3130847336.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0cb9c0fd9c4270a52c8ae7905e24f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777286247, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ah140\\OneDrive\\itu\\3_semester\\nlp\\nlp_project\\project\\wandb\\run-20241122_130149-twlp7lgy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ah140797/huggingface/runs/twlp7lgy' target=\"_blank\">first_try</a></strong> to <a href='https://wandb.ai/ah140797/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ah140797/huggingface' target=\"_blank\">https://wandb.ai/ah140797/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ah140797/huggingface/runs/twlp7lgy' target=\"_blank\">https://wandb.ai/ah140797/huggingface/runs/twlp7lgy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7a872816014e8e922ed1b3a874ebcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0439, 'grad_norm': 2.877178430557251, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.33}\n",
      "{'loss': 6.9932, 'grad_norm': 2.752091407775879, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.67}\n",
      "{'loss': 6.9209, 'grad_norm': 2.731001853942871, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\model_BPE_1000_1000.json\\checkpoint-3)... Done. 5.4s\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.6872, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'train_loss': 6.98600435256958, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=6.98600435256958, metrics={'train_runtime': 270.6872, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'total_flos': 6358582296576.0, 'train_loss': 6.98600435256958, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(encode_with_truncation, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{model_file}\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type='linear',\n",
    "    num_train_epochs=3000,\n",
    "    max_steps = 3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    #load_best_model_at_end=True,\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=1, \n",
    "    use_cpu=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name='first_try'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ah140\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for perplexity contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/perplexity/perplexity.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " torch.Size([3, 5, 10])\n",
      "\n",
      "Labels:\n",
      " torch.Size([3, 5])\n",
      "\n",
      "Computed Metrics: {'perplexity': 20.857810974121094}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the perplexity metric (or any other metric, e.g., accuracy)\n",
    "metric = load_metric(\"perplexity\")\n",
    "\n",
    "# Define a compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    greedy_prediction = torch.argmax(logits, dim=-1)\n",
    "    # Compute loss using cross-entropy\n",
    "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction=\"mean\")\n",
    "    # Calculate perplexity as exp(loss)\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    return {\"perplexity\": perplexity.item()}\n",
    "\n",
    "# Simulate some logits and labels for a batch of predictions\n",
    "# Assuming a vocab size of 10 and a sequence length of 5 for this example\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "vocab_size = 10\n",
    "\n",
    "# Random logits from a model's output (e.g., from a language model)\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Simulated labels (true values) for the same batch and sequence length\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Print logits and labels for reference\n",
    "print(\"Logits:\\n\", logits.shape)\n",
    "print(\"\\nLabels:\\n\", labels.shape)\n",
    "\n",
    "# Call the compute_metrics function to calculate perplexity\n",
    "metrics = compute_metrics((logits, labels))\n",
    "\n",
    "# Print the computed perplexity\n",
    "print(\"\\nComputed Metrics:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedNLP_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
